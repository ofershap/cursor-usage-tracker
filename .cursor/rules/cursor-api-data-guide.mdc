---
description: What Cursor API metrics actually mean, what's reliable, what's misleading, and how to interpret the data correctly
globs: "**/*.{ts,tsx}"
alwaysApply: true
---

# Cursor API Data Interpretation Guide

This file documents non-obvious behaviors, gotchas, and correct interpretations of data from the Cursor Enterprise APIs. Built from real-world analysis of team data and cross-referencing with Cursor docs, forum posts, and community tools.

## The Three APIs

Cursor provides three separate APIs, each with its own API key:

1. **Admin API** - Members, daily usage aggregates, spending, usage events, billing groups
2. **Analytics API** - Team-level and per-user analytics (DAU, models, agent edits, tabs, MCP, etc.)
3. **AI Code Tracking API** - Per-commit breakdown of AI vs human code (NOT currently collected by us)

We currently use Admin API + Analytics API. The AI Code Tracking API would give us per-commit granularity but we haven't integrated it yet.

## Critical: What `totalLinesAdded` Actually Measures

The Admin API's `/teams/daily-usage-data` returns `totalLinesAdded` and `totalLinesDeleted`. These are **NOT commit-based metrics**. They track all lines changed in the editor during a day, regardless of source.

This means `totalLinesAdded` includes:
- Lines written by the agent and applied to files
- Lines from tab completions
- Lines from composer suggestions
- Lines the developer typed manually
- Lines from paste operations, refactoring tools, etc.

**`acceptedLinesAdded`** is a subset - it only counts lines from explicitly accepted AI suggestions (tab accepts, composer accepts). But here's the critical gap:

### Agent Mode Does NOT Reliably Count as "Accepted Lines"

When the agent writes code directly to files (which is the default behavior in agent mode), those lines show up in `totalLinesAdded` but may NOT show up in `acceptedLinesAdded`. The agent applies changes directly to files without going through the explicit "suggest -> accept" flow that tab completions and composer use.

Evidence from our data:
- Users with 100% agent mode (0 composer, 0 tabs) show `acceptedLinesAdded` as a small fraction of `totalLinesAdded`
- Users like arielki: 77,830 lines_added, 0 accepted_lines, 171 agent_requests - impossible if agent lines counted as accepted
- The `totalApplies` and `totalAccepts` fields track the agent diff review flow (apply = agent showed a diff, accept = user accepted it), but the actual lines written by the agent to files are broader than just these diffs

**Bottom line: You CANNOT use `acceptedLinesAdded / totalLinesAdded` as "% of code written by AI" for agent-heavy users. It will massively undercount AI contribution.**

### The AI Code Tracking API Has Better Data

The separate `/analytics/ai-code/commits` endpoint (AI Code Tracking API) provides per-commit breakdown:
- `tabLinesAdded` - lines from tab completions
- `composerLinesAdded` - lines from composer/agent
- `nonAiLinesAdded` - manually written lines

These sum to `totalLinesAdded`. This is the correct way to measure AI vs human code. However:
- Only tracks code committed through Cursor's Source Control panel
- Commits made via external git tools (terminal, VS Code, etc.) are NOT tracked
- We don't currently collect this data

## What `totalApplies`, `totalAccepts`, `totalRejects` Mean

These track the **agent diff review flow**:
- `totalApplies` - Number of times the agent presented a code diff to the user
- `totalAccepts` - Number of times the user accepted the diff
- `totalRejects` - Number of times the user explicitly rejected the diff

The gap between `totalApplies` and `totalAccepts + totalRejects` represents diffs that were dismissed without explicit action (user moved on, started a new request, etc.).

Note: In YOLO/auto-apply mode, the agent writes directly to files without showing diffs. These changes may not increment `totalApplies` at all, which means some users will show low applies despite heavy agent usage.

## Request Types and Billing

### Request Fields in Daily Usage
- `agentRequests` - Number of agent mode requests (the primary interaction mode)
- `composerRequests` - Number of composer requests (older feature, being phased out)
- `chatRequests` - Number of chat/ask mode requests
- `usageBasedReqs` - Requests that consumed usage-based billing (cost real money beyond plan)
- `subscriptionIncludedReqs` - Requests covered by the subscription (we don't store this)
- `cmdkUsages` - Inline edit (Cmd+K) usages (we don't store this)
- `bugbotUsages` - Bugbot code review usages (we don't store this)

### How Billing Works
Cursor uses **token-based billing**, not fixed per-request pricing.
For current plan details and pricing, see: https://cursor.com/pricing

Key concepts that affect how we interpret the data:
- Each plan includes a monthly usage allowance per user
- Usage is calculated based on the model's public API token price plus a Cursor surcharge
- Once the included allowance is exhausted, additional usage is billed at the same rate
- `spend_cents` in the spending API = total cost including the included portion
- `included_spend_cents` = the portion covered by the plan
- Actual overage = `spend_cents - included_spend_cents`

### Model Pricing
Cursor charges at provider list prices (Anthropic, OpenAI, Google, xAI) plus a Teams/Enterprise surcharge of $0.25/1M total tokens. Max mode adds +20% on top. Auto mode has fixed blended rates. There is no public API for pricing tables — the canonical source is `cursor.com/docs/models`. Per-model token prices are NOT needed in our code because `usage_events.total_cents` already has the computed cost per request.

### Model Cost Drivers
Model choice is the PRIMARY cost driver. The specific models change over time, but the cost principles are stable:

- **Standard models** - Default context window, most cost-effective
- **Thinking variants** - Generate internal reasoning tokens before responding. These count as output tokens (which cost more than input), so a thinking request can easily cost 3-5x more
- **Max mode variants** - Much larger context window (e.g. 1M vs 200k). More input tokens per request means higher cost
- **Max + thinking** - Most expensive combination

For current model list and capabilities, see: https://cursor.com/docs/models

### The `fast_premium_requests` Field
Legacy field from the old fixed-pricing model. It's NOT the current billing mechanism but is still returned by the API. Don't rely on it for cost analysis.

## What We Can and Cannot Reliably Measure

### Reliable (use with confidence)
- `agentRequests`, `composerRequests`, `chatRequests` - accurate request counts per mode
- `spend_cents` and `included_spend_cents` - accurate billing data
- `totalTabsShown` and `tabsAccepted` - accurate tab completion metrics
- `mostUsedModel` - accurate per-day model preference
- `isActive` - whether the user was active that day
- `clientVersion` - what Cursor version they're on

### Unreliable or Misleading (use with caveats)
- `totalLinesAdded` - includes ALL editor changes, not just AI or committed code
- `acceptedLinesAdded` - undercounts AI contribution for agent-mode users
- `acceptedLinesAdded / totalLinesAdded` as "AI %" - WRONG for agent users, only valid for tab/composer-heavy users
- `totalApplies` - may undercount in YOLO/auto-apply mode
- `usageBasedReqs` vs `agentRequests` - the gap between these is unclear; likely `usageBasedReqs` counts only requests that consumed usage-based billing

### Now Collected
- Per-request token/cost data from `/teams/filtered-usage-events` - gives per-model cost breakdown per user. Stored in `usage_events` table. Collected incrementally (since last timestamp).
- Command adoption from `/analytics/team/commands` - team-level command usage. Stored in `analytics_commands` table.
- Plan mode adoption from `/analytics/team/plans` - plan mode usage by model. Stored in `analytics_plans` table.
- Per-user MCP tool usage from `/analytics/by-user/mcp` - which MCP tools each user uses. Stored in `analytics_user_mcp` table.
- Per-user command usage from `/analytics/by-user/commands` - which commands each user uses. Stored in `analytics_user_commands` table.

### Not Currently Collected (but available)
- AI Code Tracking data from `/analytics/ai-code/commits` - would give us accurate AI vs human line attribution
- Per-user breakdowns from `/analytics/by-user/*` endpoints for: agent-edits, tabs, models, plans, ask-mode, client-versions, top-file-extensions (we collect mcp and commands per-user, but not these others)
- Leaderboard from `/analytics/team/leaderboard` - ranks users by tab accepts and agent edits. We chose NOT to collect this because it introduces a third ranking system that conflicts with our own spend_rank and activity_rank, confusing stakeholders.
- `cmdkUsages`, `subscriptionIncludedReqs`, `apiKeyReqs`, `bugbotUsages` - available in daily usage but not stored
- Audit logs from `/teams/audit-logs` - login events, settings changes, security events

## Derived Metrics We Use

### $/req (Cost Per Request)
`spend_cents / agent_requests / 100`

Useful but context-dependent. High $/req can mean:
- Expensive model choice (opus-max vs opus-high) - most common cause
- Long context sessions (large codebases)
- Thinking mode usage (more output tokens)
- NOT necessarily inefficiency

### Accept Rate
`total_accepts / total_applies * 100`

What it tells you: How often the user accepts agent-suggested diffs.
What it does NOT tell you: Quality of work, developer skill, or AI effectiveness.
Low accept rate could mean: picky reviewer (good), bad prompting (fixable), or wrong tool for the task (neutral).

### Lines Per Request
`lines_added / agent_requests`

Highly task-dependent. A debugging session produces 0 lines. A scaffolding task produces 500. Not a quality metric.

## Daily Spend Data Sources

`usage_events` (from `/teams/filtered-usage-events`) is the most reliable source for daily spend data. It has per-request cost (`total_cents`) with full billing cycle history and no retention window. `daily_spend` (from `/teams/groups` billing groups API) has only ~2 days retention and systematically underreports compared to `usage_events`.

The dashboard daily spend chart uses `usage_events` as the primary source, falling back to `daily_spend` only when the `usage_events` table is empty (e.g., a fresh install that hasn't collected events yet). The chart marks the last 2 days as "provisional" since spend data for today/yesterday may still be accumulating.

## Conversation Insights (Dashboard-Only)

The Cursor web dashboard has a "Conversation Insights" page (`cursor.com/dashboard?tab=conversation-insights`) that shows Work Type (KTLO/Feature/Bug), Intent Distribution (Write Code/Ask/Task Automation/Plan), Categories (Bug Fix/Configuration/Feature/Refactor), Task Complexity, and Prompt Specificity. This data is computed server-side from conversation content using AI analysis. There is NO API endpoint for it — it is a dashboard-only enterprise feature.

## Complete Analytics API Endpoint List

### Team-level endpoints (all collected)
- `/analytics/team/dau` — daily active users (+ CLI, Cloud Agent, BugBot DAU)
- `/analytics/team/models` — model usage breakdown per day
- `/analytics/team/agent-edits` — diffs suggested/accepted/rejected
- `/analytics/team/tabs` — tab autocomplete metrics
- `/analytics/team/mcp` — MCP tool adoption
- `/analytics/team/top-file-extensions` — file types
- `/analytics/team/client-versions` — version distribution
- `/analytics/team/commands` — command adoption
- `/analytics/team/plans` — plan mode adoption
- `/analytics/team/ask-mode` — ask mode adoption (not collected)
- `/analytics/team/leaderboard` — user rankings by AI usage (not collected — see note above)

### By-user endpoints (paginated, data keyed by email)
- `/analytics/by-user/mcp` — per-user MCP tool usage (collected)
- `/analytics/by-user/commands` — per-user command usage (collected)
- `/analytics/by-user/agent-edits` — per-user agent edits (not collected)
- `/analytics/by-user/tabs` — per-user tab usage (not collected)
- `/analytics/by-user/models` — per-user model usage (not collected — covered by daily_usage)
- `/analytics/by-user/plans` — per-user plan mode (not collected)
- `/analytics/by-user/ask-mode` — per-user ask mode (not collected)
- `/analytics/by-user/client-versions` — per-user versions (not collected — covered by daily_usage)
- `/analytics/by-user/top-file-extensions` — per-user file types (not collected)

## Critical: Billing Groups API Daily Spend Retention

The `/teams/groups` endpoint returns `dailySpend` per member, but this data has a **very short retention window — approximately 2 days**. Older daily spend data is dropped from the API response entirely.

This means:
- If you don't collect at least once per day, you will permanently lose daily spend granularity for missed days
- Early-day collections capture incomplete data (spend accumulates throughout the day)
- The `upsertDailySpend` function uses `MAX(existing, new)` to prevent regressions from partial data overwriting complete data
- Ideal collection frequency: at least twice daily (e.g. midday + end of day) to capture most of each day's spend before it falls off the API
- The dashboard marks the last 2 days as "partial (API lag)" since spend data may not be fully settled yet
